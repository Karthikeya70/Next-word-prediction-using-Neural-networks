# Next Word Prediction using MLP

- Built a **word-level next-word prediction model** using a Multi-Layer Perceptron (MLP).
- Used *The Adventures of Sherlock Holmes* dataset from **Project Gutenberg [https://www.gutenberg.org/files/1661/1661-0.txt](https://www.gutenberg.org/files/1661/1661-0.txt)**.
- Cleaned the raw text by:
  - Converting all text to lowercase
  - Removing special characters
- Tokenized the cleaned text into **words**.
- Created a vocabulary of unique words and mapped:
  - Words → numerical indices
  - Indices → words
- Generated training data using a **sliding context window** approach.
- Each training example consists of:
  - Input: fixed number of previous words
  - Output: the next word
- Implemented a **custom Dataset class** to dynamically generate training samples.
- Used **PyTorch DataLoader** to:
  - Load data in batches
  - Shuffle training examples
- Designed an MLP model with:
  - Embedding layer to represent word meanings
  - Fully connected layers for learning patterns
  - Dropout layer to reduce overfitting
- Trained the model using:
  - Cross Entropy Loss
  - Adam optimizer
- Evaluated performance using a validation dataset.
- Implemented a prediction function to return the **top-k next-word predictions** with probabilities.
- Built a **Streamlit web application** to interactively test the trained model.
- Observed and analyzed overfitting due to model simplicity and fixed context size.
